import os, json, re, random, sys
from collections import Counter, defaultdict

IN = "github_repos.json"
OUT = "service_discovery_dataset.json"

TARGET = int(os.getenv("DATASET_SIZE", "250"))
MIN_OK = int(os.getenv("MIN_DATASET_SIZE", "210"))

random.seed(42)

SENT_MIN = 90     # Ø£Ù‚Ù„ Ø·ÙˆÙ„ Ù„Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„Ø©
SENT_MAX = 320    # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„Ù„Ø¬Ù…Ù„Ø© Ù„Ù„Ù€ query/relevance
GT_MIN   = 180    # Ø£Ù‚Ù„ Ø·ÙˆÙ„ Ù„Ù„Ù€ ground_truth
GT_MAX   = 520    # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„Ù„Ù€ ground_truth

def normalize_ws(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def split_sentences(text: str):
    if not text:
        return []
    # ØªÙ‚Ø³ÙŠÙ… Ø¨Ø³ÙŠØ· Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¬Ù…Ù„
    parts = re.split(r'(?<=[\.\?\!])\s+|\n+', text)
    sents = []
    seen = set()
    for p in parts:
        p = normalize_ws(p)
        if len(p) < SENT_MIN:
            continue
        if len(p) > 600:  # Ù‚Øµ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§
            p = p[:600].rstrip() + "."
        if p and p not in seen:
            seen.add(p)
            sents.append(p)
    return sents

def tokens(text: str):
    return set(re.findall(r"[a-zA-Z][a-zA-Z0-9\-]+", (text or "").lower()))

def jaccard(a, b):
    if not a or not b:
        return 0.0
    inter = len(a & b)
    if inter == 0:
        return 0.0
    union = len(a | b)
    return inter / union if union else 0.0

def build_long_sentence(sentences, target_min=GT_MIN, target_max=GT_MAX):
    # Ø¯Ù…Ø¬ 2-3 Ø¬ÙÙ…Ù„ Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙÙŠ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ø·ÙˆÙŠÙ„Ø© Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø·Ø§Øª
    chosen = []
    total = 0
    for s in sentences[:5]:  # Ø¬Ø±Ù‘Ø¨ Ø£ÙˆÙ„ 5 Ø¬ÙÙ…Ù„ Ø·ÙˆÙŠÙ„Ø©
        if len(s) > SENT_MAX:
            continue
        chosen.append(s.rstrip("."))
        total = len(", ".join(chosen))
        if target_min <= total <= target_max:
            break
    if not chosen:
        # fallback: Ø®ÙØ¯ Ø£Ø·ÙˆÙ„ Ø¬Ù…Ù„Ø© Ù…Ø³Ù…ÙˆØ­Ø© ÙˆÙƒØ±Ø± Ø¬Ù…Ù„Ø© ØªØ§Ù†ÙŠØ© Ù„Ùˆ Ù…ØªØ§Ø­
        candidates = [s.rstrip(".") for s in sentences if len(s) <= SENT_MAX]
        if not candidates:
            return ""
        chosen = [candidates[0]]
        if len(candidates) > 1:
            chosen.append(candidates[1])
    merged = ", ".join(chosen)
    # Ø¶Ù…Ù‡Ù… ÙƒÙ€ "Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ø·ÙˆÙŠÙ„Ø©"
    return (merged + ".").strip()

def pick_sentence_from_repo(repo, used_set, avoid_texts):
    # Ø§Ø®ØªØ± Ø¬Ù…Ù„Ø© â€œÙ…Ù…ÙŠØ²Ø© ÙˆØ·ÙˆÙŠÙ„Ø©â€ Ù…Ù† ÙˆØµÙ/README
    sents = split_sentences(repo.get("description","")) + split_sentences(repo.get("readme",""))
    random.shuffle(sents)
    for s in sents:
        if len(s) > SENT_MAX:  # Ø®Ù„ÙŠ Ø¬Ù…Ù„ relevance/queries Ù…Ø¹Ù‚ÙˆÙ„Ø© Ø§Ù„Ø·ÙˆÙ„
            continue
        if s in used_set:
            continue
        if any(s == a or s in a or a in s for a in avoid_texts):
            continue
        used_set.add(s)
        return s
    return None

def main():
    if not os.path.exists(IN):
        print(f"âŒ {IN} not found. Run fetch_github_data.py first.")
        sys.exit(1)

    with open(IN, "r", encoding="utf-8") as f:
        repos = json.load(f)

    # ÙÙ„ØªØ±Ø© Repos Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ø§ Ù†Øµ ÙƒÙØ§ÙŠØ©
    filtered = []
    for r in repos:
        text = normalize_ws(r.get("text",""))
        desc = normalize_ws(r.get("description",""))
        rd   = normalize_ws(r.get("readme",""))
        if len(desc) + len(rd) < 200:   # Ù„Ø§Ø²Ù… ÙŠØ¨Ù‚Ù‰ ÙÙŠÙ‡ Ù…Ø§Ø¯Ø© Ù†ØµÙŠØ© ÙƒÙØ§ÙŠØ©
            continue
        r["text"] = text
        filtered.append(r)

    random.shuffle(filtered)
    print(f"ğŸ“¦ candidates after filtering: {len(filtered)}")

    # Ø¬Ù‡Ù‘Ø² tokens Ù„ÙƒÙ„ repo
    tok = [tokens(r["text"]) for r in filtered]
    by_org = defaultdict(list)
    for i, r in enumerate(filtered):
        by_org[r["org"]].append(i)

    # Ø¹Ù†ÙˆØ§ÙŠÙ† uniqueness
    used_queries = set()
    used_ground_truth = set()
    used_relevance = set()

    dataset = []
    for idx, r in enumerate(filtered):
        if len(dataset) >= TARGET:
            break

        org = r["org"]
        name = r["full_name"]

        # 1) query: Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„ÙˆØµÙ/README â€œÙ…Ù…ÙŠØ²Ø©â€
        cand_sents = split_sentences(r.get("description","")) + split_sentences(r.get("readme",""))
        if not cand_sents:
            continue
        # Ø±ØªÙ‘Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø·ÙˆÙ„ (Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹) Ù„Ø¶Ù…Ø§Ù† â€œØ³Ø¤Ø§Ù„ Ø·ÙˆÙŠÙ„â€
        cand_sents.sort(key=len, reverse=True)
        query = None
        for s in cand_sents:
            if SENT_MIN <= len(s) <= SENT_MAX and s not in used_queries:
                query = s
                break
        if not query:
            continue
        used_queries.add(query)

        # 2) description: ÙˆØµÙ GitHub ÙƒÙ…Ø§ Ù‡Ùˆ (ÙˆÙ„Ùˆ ÙØ§Ø¶ÙŠ Ù†Ø§Ø®Ø¯ Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø© Ù…Ù† README)
        description = normalize_ws(r.get("description",""))
        if not description:
            # Ø®Ø¯ Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„README
            for s in cand_sents[::-1]:
                if len(s) <= SENT_MAX:
                    description = s
                    break
        if not description:
            # Ù…Ø§ ÙÙŠØ´ ÙˆØµÙ Ù…Ù†Ø§Ø³Ø¨
            used_queries.discard(query)
            continue

        # 3) ground_truth: Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ø·ÙˆÙŠÙ„Ø© Ù…Ù† Ø¯Ù…Ø¬ Ø¬ÙÙ…Ù„ Ø­Ù‚ÙŠÙ‚ÙŠØ©
        long_sents = cand_sents[:]
        gt = build_long_sentence(long_sents, target_min=GT_MIN, target_max=GT_MAX)
        if not gt or gt in used_ground_truth or gt == query:
            # Ø¬Ø±Ù‘Ø¨ ØªÙƒÙˆÙŠÙ† Ù…Ø®ØªÙ„Ù
            random.shuffle(long_sents)
            gt = build_long_sentence(long_sents, target_min=GT_MIN, target_max=GT_MAX)
        if not gt or gt in used_ground_truth or gt == query:
            used_queries.discard(query)
            continue
        used_ground_truth.add(gt)

        # 4) relevance selection via similarity (Ù†ØµÙŠ ÙÙ‚Ø·)
        t_self = tok[idx]

        # Ø­Ø¶Ù‘Ø± Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
        # high: Ù†ÙØ³ org Ø¨Ø£Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
        high_candidates = []
        for j in by_org[org]:
            if j == idx:
                continue
            sim = jaccard(t_self, tok[j])
            high_candidates.append((sim, j))
        high_candidates.sort(reverse=True, key=lambda x: x[0])

        # medium: Ù…Ù† Ù…Ù†Ø¸Ù…Ø§Øª Ø£Ø®Ø±Ù‰ Ø¨ØªØ´Ø§Ø¨Ù‡ Ù…ØªÙˆØ³Ø·
        medium_candidates = []
        low_candidates = []
        for j, t in enumerate(tok):
            if j == idx:
                continue
            s = jaccard(t_self, t)
            if filtered[j]["org"] != org:
                if 0.06 <= s <= 0.30:
                    medium_candidates.append((s, j))
                elif s < 0.02:
                    low_candidates.append((s, j))
        random.shuffle(medium_candidates)
        random.shuffle(low_candidates)

        # Ø§Ø®ØªØ§Ø± Ø¬ÙÙ…Ù„ Ù…Ù…ÙŠØ²Ø© (Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±) Ù„Ù„Ù€ 4/4/4
        avoid = {query, gt, description}
        avoid_texts = list(avoid)

        def pick_many(cands, needed):
            chosen_texts = []
            seen_names = set()
            for _, j in cands:
                rr = filtered[j]
                if rr["full_name"] in seen_names:
                    continue
                sent = pick_sentence_from_repo(rr, used_relevance, avoid_texts)
                if not sent:
                    continue
                chosen_texts.append(sent)
                seen_names.add(rr["full_name"])
                if len(chosen_texts) == needed:
                    break
            return chosen_texts

        high = pick_many(high_candidates, 4)
        if len(high) < 4:
            # Ù„Ùˆ Ø§Ù„Ù…Ù†Ø¸Ù…Ø© ØµØºÙŠØ±Ø©ØŒ ÙˆØ³Ù‘Ø¹ Ø§Ù„Ø¯Ø§Ø¦Ø±Ø© Ø¨Ø£Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø¹Ø¨Ø± Ø§Ù„ÙƒÙ„
            others = [(jaccard(t_self, tok[j]), j) for j in range(len(tok)) if j != idx]
            others.sort(reverse=True, key=lambda x: x[0])
            extra = pick_many(others, 4 - len(high))
            high.extend(extra[:max(0, 4 - len(high))])

        medium = pick_many(medium_candidates, 4)
        if len(medium) < 4:
            # ÙˆØ³Ù‘Ø¹ Ø±ÙŠÙ†Ú† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§
            fallback_med = []
            for j, t in enumerate(tok):
                if j == idx or filtered[j]["org"] == org:
                    continue
                s = jaccard(t_self, t)
                if 0.03 <= s < 0.06 or 0.30 < s <= 0.45:
                    fallback_med.append((s, j))
            random.shuffle(fallback_med)
            extra = pick_many(fallback_med, 4 - len(medium))
            medium.extend(extra[:max(0, 4 - len(medium))])

        low = pick_many(low_candidates, 4)
        if len(low) < 4:
            # Ø®ÙØ¯ Ø§Ù„Ø£Ù‚Ù„ ØªØ´Ø§Ø¨Ù‡Ù‹Ø§ Ø¹Ø§Ù„Ù…ÙŠÙ‹Ø§
            all_sorted = sorted([(jaccard(t_self, tok[j]), j) for j in range(len(tok)) if j != idx], key=lambda x: x[0])
            extra = pick_many(all_sorted, 4 - len(low))
            low.extend(extra[:max(0, 4 - len(low))])

        # Ù„Ùˆ Ø£ÙŠ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø´ Ù…ÙƒØªÙ…ÙÙ„Ø© 4 â†’ ØªØ®Ø·Ù‘ÙŠ Ø§Ù„Ø³Ø¬Ù„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (Ø¹Ù„Ø´Ø§Ù† Ø§Ù„ÙØ§Ù„ÙŠØ¯ÙŠØªÙˆØ±)
        if not (len(high) == len(medium) == len(low) == 4):
            # Ø§Ø±Ø¬Ø¹ Ø§Ù„Ù€ query/gt Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¹Ø´Ø§Ù† Ù…Ù…ÙƒÙ† Ù†Ø¹ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§
            used_queries.discard(query)
            used_ground_truth.discard(gt)
            continue

        example = {
            "query": query,
            "description": description,
            "ground_truth": gt,
            "high_relevance": high,
            "medium_relevance": medium,
            "low_relevance": low
        }
        dataset.append(example)

    if len(dataset) < MIN_OK:
        raise ValueError(f"âš ï¸ Built only {len(dataset)} examples (<{MIN_OK}). Increase orgs or pages.")

    with open(OUT, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    print(f"âœ… Built {len(dataset)} examples â†’ {OUT}")
    # quick assert
    bad = [i for i, x in enumerate(dataset, 1) if not (len(x["high_relevance"])==len(x["medium_relevance"])==len(x["low_relevance"])==4)]
    if bad:
        raise RuntimeError(f"Validator pre-check failed for entries: {bad[:10]}")

if __name__ == "__main__":
    main()
